{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b2159e",
   "metadata": {},
   "source": [
    "# Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f0aa76f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d87d8f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3f4e11e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7842, 43)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "test_dir = \"./data/test\"\n",
    "test_ds = ImageFolder(test_dir)\n",
    "len(test_ds), len(test_ds.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183bb81c",
   "metadata": {},
   "source": [
    "## cVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ded00368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConditionalVariationalAutoencoder(\n",
       "  (encoder_conv): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (fc_mu): Linear(in_features=2091, out_features=100, bias=True)\n",
       "  (fc_logvar): Linear(in_features=2091, out_features=100, bias=True)\n",
       "  (decoder_fc): Linear(in_features=143, out_features=2048, bias=True)\n",
       "  (decoder_conv_transpose): Sequential(\n",
       "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (5): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gnn import ConditionalVariationalAutoencoder as cVAE\n",
    "from gnn import cVAETrainer\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "IMG_CHANNELS = 3\n",
    "latent_dim = 100\n",
    "num_classes = len(test_ds.classes)\n",
    "\n",
    "# model\n",
    "cvae_model = cVAE(IMG_CHANNELS, num_classes, latent_dim=latent_dim).to(device)\n",
    "\n",
    "cvae_model.load_state_dict(torch.load(\"gnn/weights/cvae_model.pth\", map_location=torch.device('cuda')))\n",
    "cvae_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a65cbb",
   "metadata": {},
   "source": [
    "## Big cVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "68ed8001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigConditionalVariationalAutoencoder(\n",
       "  (encoder_conv): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (9): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (12): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (fc_mu): Linear(in_features=2091, out_features=256, bias=True)\n",
       "  (fc_logvar): Linear(in_features=2091, out_features=256, bias=True)\n",
       "  (decoder_fc): Linear(in_features=299, out_features=2048, bias=True)\n",
       "  (decoder_bn_fc): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (decoder_relu_fc): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (decoder_conv_transpose): Sequential(\n",
       "    (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (6): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (9): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (10): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gnn import BigConditionalVariationalAutoencoder as cVAE\n",
    "from gnn import BigcVAETrainer as cVAETrainer\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "IMG_CHANNELS = 3\n",
    "latent_dim = 256\n",
    "num_classes = len(test_ds.classes)\n",
    "\n",
    "# model\n",
    "cvae_model = cVAE(IMG_CHANNELS, num_classes, latent_dim=latent_dim).to(device)\n",
    "\n",
    "cvae_model.load_state_dict(torch.load(\"gnn/weights/cvae_model_big.pth\", map_location=torch.device('cuda')))\n",
    "cvae_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ed8dfd",
   "metadata": {},
   "source": [
    "## GAN \n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2cc387c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239f5b07",
   "metadata": {},
   "source": [
    "## Generating tensor and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "14014009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]),\n",
       " dict_values([5, 57, 57, 36, 50, 47, 11, 37, 36, 37, 51, 34, 54, 55, 20, 16, 11, 28, 31, 5, 9, 8, 10, 13, 7, 38, 15, 6, 14, 7, 11, 20, 6, 18, 11, 31, 10, 5, 53, 8, 9, 6, 6]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_test_samples = sum(class_counts.values())\n",
    "class_ratios = {cls: count / total_test_samples for cls, count in class_counts.items()}\n",
    "\n",
    "# ile próbek z każdej klasy w puli 1000\n",
    "samples_per_class = {cls: int(round(ratio * num_samples)) for cls, ratio in class_ratios.items()}\n",
    "samples_per_class.keys(), samples_per_class.values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bb1c08cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init samples: 999\n",
      "Adjusted samples: 1000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from torchvision.utils import save_image\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "results_id = time.time()\n",
    "\n",
    "\n",
    "results_dir_jpg = f\"./cvae_results/jpg/{results_id}\"\n",
    "results_dir_pt = f\"./cvae_results/pt/{results_id}\"\n",
    "\n",
    "os.makedirs(results_dir_jpg, exist_ok=True)\n",
    "os.makedirs(results_dir_pt, exist_ok=True)\n",
    "\n",
    "train_mean = [0.5, 0.5, 0.5]\n",
    "train_std = [0.5, 0.5, 0.5]\n",
    "\n",
    "\n",
    "mean_t = torch.tensor(train_mean).view(1, IMG_CHANNELS, 1, 1).to(device)\n",
    "std_t = torch.tensor(train_std).view(1, IMG_CHANNELS, 1, 1).to(device)\n",
    "class_counts = Counter(test_ds.targets)\n",
    "\n",
    "\n",
    "num_samples = 1000\n",
    "total_test_samples = sum(class_counts.values())\n",
    "class_ratios = {cls: count / total_test_samples for cls, count in class_counts.items()}\n",
    "samples_per_class = {cls: int(round(ratio * num_samples)) for cls, ratio in class_ratios.items()}\n",
    "\n",
    "print(f\"Init samples: {sum(samples_per_class.values())}\")\n",
    "# adjust to 1000\n",
    "adjustment = num_samples - sum(samples_per_class.values())\n",
    "if adjustment != 0:\n",
    "    most_common_class = max(samples_per_class, key=samples_per_class.get)\n",
    "    samples_per_class[most_common_class] += adjustment\n",
    "print(f\"Adjusted samples: {sum(samples_per_class.values())}\")\n",
    "\n",
    "\n",
    "def _save(model):\n",
    "    generated_imgs = []\n",
    "    for cls, count in samples_per_class.items():\n",
    "        for i in range(count):\n",
    "            z = torch.randn(1, latent_dim, device=device)\n",
    "            label_tensor = torch.tensor([cls], dtype=torch.long, device=device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                img = model.generate(z, label_tensor)\n",
    "\n",
    "            img = img * std_t + mean_t\n",
    "            generated_imgs.append(img.cpu().detach())\n",
    "\n",
    "            # Save the image\n",
    "            fname = os.path.join(results_dir_jpg, f\"class_{cls}_sample_{i}.jpg\")\n",
    "            save_image(img.clamp(0, 1), fname)\n",
    "\n",
    "    print(f\"Saved generated images to {results_dir_jpg}\")\n",
    "    # Save the tensor\n",
    "    generated_imgs = torch.cat(generated_imgs, dim=0)\n",
    "    return generated_imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "158d9ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating using model name:  BigConditionalVariationalAutoencoder\n",
      "Saved generated images to ./cvae_results/jpg/1747141943.95138\n",
      "Saved generated tensor to ./cvae_results/pt/1747141943.95138\n"
     ]
    }
   ],
   "source": [
    "# Evaluating\n",
    "print(\"Evaluating using model name: \", cvae_model.__class__.__name__)\n",
    "# cvae_model.eval()\n",
    "\n",
    "generated_imgs = _save(cvae_model)\n",
    "\n",
    "# save tensor\n",
    "assert generated_imgs.shape == (1000, 3, 32, 32), f\"Zły rozmiar tensora: {generated_imgs.shape}\"\n",
    "fname = os.path.join(results_dir_pt, f\"poniedzialek_matukiewicz_statkiewicz.pt\")                # TODO nazwiska i dzien sprawdzic ! ! !\n",
    "torch.save(generated_imgs, fname)\n",
    "\n",
    "print(f\"Saved generated tensor to {results_dir_pt}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e1537c",
   "metadata": {},
   "source": [
    "## FID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec764a",
   "metadata": {},
   "source": [
    "### Fid ConditionalVariationalAutoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7ce424bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:13<00:00,  1.17it/s]\n",
      "100%|██████████| 8/8 [00:05<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID: 130.60183958428928\n"
     ]
    }
   ],
   "source": [
    "from pytorch_fid.fid_score import calculate_fid_given_paths\n",
    "\n",
    "test_flat_dir = \"./data/test_flat\"\n",
    "generated_dir = results_dir_jpg\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "fid = calculate_fid_given_paths([test_flat_dir, generated_dir], batch_size, device, dims=2048, num_workers=1)\n",
    "\n",
    "print(f\"FID: {fid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb800b3",
   "metadata": {},
   "source": [
    "#### Fid BigConditionalVariationalAutoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8dade6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:13<00:00,  1.20it/s]\n",
      "100%|██████████| 8/8 [00:05<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID: 127.72886070703294\n"
     ]
    }
   ],
   "source": [
    "from pytorch_fid.fid_score import calculate_fid_given_paths\n",
    "\n",
    "test_flat_dir = \"./data/test_flat\"\n",
    "generated_dir = results_dir_jpg\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "fid = calculate_fid_given_paths([test_flat_dir, generated_dir], batch_size, device, dims=2048, num_workers=1)\n",
    "\n",
    "print(f\"FID: {fid}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
