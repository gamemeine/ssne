{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b2159e",
   "metadata": {},
   "source": [
    "# Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f0aa76f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d87d8f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f4e11e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any class folder in ./data/test.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageFolder\n\u001b[32m      3\u001b[39m test_dir = \u001b[33m\"\u001b[39m\u001b[33m./data/test\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m test_ds = \u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_ds), \u001b[38;5;28mlen\u001b[39m(test_ds.classes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/studies/ssne/.venv/lib/python3.12/site-packages/torchvision/datasets/folder.py:328\u001b[39m, in \u001b[36mImageFolder.__init__\u001b[39m\u001b[34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    320\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    321\u001b[39m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m   (...)\u001b[39m\u001b[32m    326\u001b[39m     allow_empty: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    327\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m     \u001b[38;5;28mself\u001b[39m.imgs = \u001b[38;5;28mself\u001b[39m.samples\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/studies/ssne/.venv/lib/python3.12/site-packages/torchvision/datasets/folder.py:149\u001b[39m, in \u001b[36mDatasetFolder.__init__\u001b[39m\u001b[34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    139\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    140\u001b[39m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m   (...)\u001b[39m\u001b[32m    146\u001b[39m     allow_empty: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    147\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    148\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(root, transform=transform, target_transform=target_transform)\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     classes, class_to_idx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m     samples = \u001b[38;5;28mself\u001b[39m.make_dataset(\n\u001b[32m    151\u001b[39m         \u001b[38;5;28mself\u001b[39m.root,\n\u001b[32m    152\u001b[39m         class_to_idx=class_to_idx,\n\u001b[32m   (...)\u001b[39m\u001b[32m    155\u001b[39m         allow_empty=allow_empty,\n\u001b[32m    156\u001b[39m     )\n\u001b[32m    158\u001b[39m     \u001b[38;5;28mself\u001b[39m.loader = loader\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/studies/ssne/.venv/lib/python3.12/site-packages/torchvision/datasets/folder.py:234\u001b[39m, in \u001b[36mDatasetFolder.find_classes\u001b[39m\u001b[34m(self, directory)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) -> Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[32m    208\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[32m    209\u001b[39m \n\u001b[32m    210\u001b[39m \u001b[33;03m        directory/\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    232\u001b[39m \u001b[33;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[32m    233\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/studies/ssne/.venv/lib/python3.12/site-packages/torchvision/datasets/folder.py:43\u001b[39m, in \u001b[36mfind_classes\u001b[39m\u001b[34m(directory)\u001b[39m\n\u001b[32m     41\u001b[39m classes = \u001b[38;5;28msorted\u001b[39m(entry.name \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m os.scandir(directory) \u001b[38;5;28;01mif\u001b[39;00m entry.is_dir())\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m class_to_idx = {cls_name: i \u001b[38;5;28;01mfor\u001b[39;00m i, cls_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(classes)}\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m classes, class_to_idx\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Couldn't find any class folder in ./data/test."
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "test_dir = \"./data/test\"\n",
    "test_ds = ImageFolder(test_dir)\n",
    "len(test_ds), len(test_ds.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183bb81c",
   "metadata": {},
   "source": [
    "## cVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239f5b07",
   "metadata": {},
   "source": [
    "## Generating tensor and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "14014009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]),\n",
       " dict_values([5, 57, 57, 36, 50, 47, 11, 37, 36, 37, 51, 34, 54, 55, 20, 16, 11, 28, 31, 5, 9, 8, 10, 13, 7, 38, 15, 6, 14, 7, 11, 20, 6, 18, 11, 31, 10, 5, 53, 8, 9, 6, 6]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_test_samples = sum(class_counts.values())\n",
    "class_ratios = {cls: count / total_test_samples for cls, count in class_counts.items()}\n",
    "\n",
    "# ile próbek z każdej klasy w puli 1000\n",
    "samples_per_class = {cls: int(round(ratio * num_samples)) for cls, ratio in class_ratios.items()}\n",
    "samples_per_class.keys(), samples_per_class.values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a0c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnn.models import Generator, Discriminator\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "latent_dim = 128\n",
    "num_classes = len(test_ds.classes)\n",
    "\n",
    "# model\n",
    "\n",
    "generator = Generator(latent_dim, num_classes).to(device)\n",
    "generator.load_state_dict(torch.load(\"1747144428.030544/generator.pth\"))\n",
    "generator.eval()\n",
    "\n",
    "discriminator = Discriminator(latent_dim, num_classes).to(device)\n",
    "discriminator.load_state_dict(torch.load(\"1747144428.030544/discriminator.pth\"))\n",
    "discriminator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bb1c08cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init samples: 999\n",
      "Adjusted samples: 1000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from torchvision.utils import save_image\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "results_id = time.time()\n",
    "\n",
    "\n",
    "results_dir_jpg = f\"./cvae_results/jpg/{results_id}\"\n",
    "results_dir_pt = f\"./cvae_results/pt/{results_id}\"\n",
    "\n",
    "os.makedirs(results_dir_jpg, exist_ok=True)\n",
    "os.makedirs(results_dir_pt, exist_ok=True)\n",
    "\n",
    "train_mean = [0.5, 0.5, 0.5]\n",
    "train_std = [0.5, 0.5, 0.5]\n",
    "\n",
    "\n",
    "mean_t = torch.tensor(train_mean).view(1, IMG_CHANNELS, 1, 1).to(device)\n",
    "std_t = torch.tensor(train_std).view(1, IMG_CHANNELS, 1, 1).to(device)\n",
    "class_counts = Counter(test_ds.targets)\n",
    "\n",
    "\n",
    "num_samples = 1000\n",
    "total_test_samples = sum(class_counts.values())\n",
    "class_ratios = {cls: count / total_test_samples for cls, count in class_counts.items()}\n",
    "samples_per_class = {cls: int(round(ratio * num_samples)) for cls, ratio in class_ratios.items()}\n",
    "\n",
    "print(f\"Init samples: {sum(samples_per_class.values())}\")\n",
    "# adjust to 1000\n",
    "adjustment = num_samples - sum(samples_per_class.values())\n",
    "if adjustment != 0:\n",
    "    most_common_class = max(samples_per_class, key=samples_per_class.get)\n",
    "    samples_per_class[most_common_class] += adjustment\n",
    "print(f\"Adjusted samples: {sum(samples_per_class.values())}\")\n",
    "\n",
    "\n",
    "def _save(model):\n",
    "    generated_imgs = []\n",
    "    for cls, count in samples_per_class.items():\n",
    "        for i in range(count):\n",
    "            z = torch.randn(1, latent_dim, device=device)\n",
    "            label_tensor = torch.tensor([cls], dtype=torch.long, device=device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                img = model.generate(z, label_tensor)\n",
    "\n",
    "            img = img * std_t + mean_t\n",
    "            generated_imgs.append(img.cpu().detach())\n",
    "\n",
    "            # Save the image\n",
    "            fname = os.path.join(results_dir_jpg, f\"class_{cls}_sample_{i}.jpg\")\n",
    "            save_image(img.clamp(0, 1), fname)\n",
    "\n",
    "    print(f\"Saved generated images to {results_dir_jpg}\")\n",
    "    # Save the tensor\n",
    "    generated_imgs = torch.cat(generated_imgs, dim=0)\n",
    "    return generated_imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "158d9ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating using model name:  BigConditionalVariationalAutoencoder\n",
      "Saved generated images to ./cvae_results/jpg/1747141943.95138\n",
      "Saved generated tensor to ./cvae_results/pt/1747141943.95138\n"
     ]
    }
   ],
   "source": [
    "# Evaluating\n",
    "print(\"Evaluating using model name: \", cvae_model.__class__.__name__)\n",
    "# cvae_model.eval()\n",
    "\n",
    "generated_imgs = _save(cvae_model)\n",
    "\n",
    "# save tensor\n",
    "assert generated_imgs.shape == (1000, 3, 32, 32), f\"Zły rozmiar tensora: {generated_imgs.shape}\"\n",
    "fname = os.path.join(results_dir_pt, f\"poniedzialek_matukiewicz_statkiewicz.pt\")                # TODO nazwiska i dzien sprawdzic ! ! !\n",
    "torch.save(generated_imgs, fname)\n",
    "\n",
    "print(f\"Saved generated tensor to {results_dir_pt}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e1537c",
   "metadata": {},
   "source": [
    "## FID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec764a",
   "metadata": {},
   "source": [
    "### Fid ConditionalVariationalAutoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7ce424bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:13<00:00,  1.17it/s]\n",
      "100%|██████████| 8/8 [00:05<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID: 130.60183958428928\n"
     ]
    }
   ],
   "source": [
    "from pytorch_fid.fid_score import calculate_fid_given_paths\n",
    "\n",
    "test_flat_dir = \"./data/test_flat\"\n",
    "generated_dir = results_dir_jpg\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "fid = calculate_fid_given_paths([test_flat_dir, generated_dir], batch_size, device, dims=2048, num_workers=1)\n",
    "\n",
    "print(f\"FID: {fid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb800b3",
   "metadata": {},
   "source": [
    "#### Fid BigConditionalVariationalAutoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8dade6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:13<00:00,  1.20it/s]\n",
      "100%|██████████| 8/8 [00:05<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID: 127.72886070703294\n"
     ]
    }
   ],
   "source": [
    "from pytorch_fid.fid_score import calculate_fid_given_paths\n",
    "\n",
    "test_flat_dir = \"./data/test_flat\"\n",
    "generated_dir = results_dir_jpg\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "fid = calculate_fid_given_paths([test_flat_dir, generated_dir], batch_size, device, dims=2048, num_workers=1)\n",
    "\n",
    "print(f\"FID: {fid}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
