{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c404ae5",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "004d49ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun  8 23:55:46 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.02              Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1660 Ti     On  |   00000000:1C:00.0  On |                  N/A |\n",
      "|  0%   51C    P8             19W /  130W |    2415MiB /   6144MiB |     23%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A       380      G   /Xwayland                                   N/A      |\n",
      "|    0   N/A  N/A     62836      C   /python3.13                                 N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10e3abfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/matimat/LLM/.venv/lib/python3.13/site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from datasets) (2.2.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.16)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.19.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2890073f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /home/matimat/LLM/.venv/lib/python3.13/site-packages (0.46.0)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from bitsandbytes) (2.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from bitsandbytes) (2.2.4)\n",
      "Requirement already satisfied: filelock in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes) (4.13.1)\n",
      "Requirement already satisfied: networkx in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/matimat/LLM/.venv/lib/python3.13/site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3113fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96ccc9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(\"Cuda is not available. Exiting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39f2dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_ID = \"EleutherAI/gpt-neo-125M\" # TODO: check few different models\n",
    "LORA_MODEL_OUTPUT_DIR = \"./hate-speech-lora-model\"\n",
    "TRAIN_FILE = \"data/hate_train.csv\"\n",
    "TEST_FILE = \"data/hate_test_data.txt\"\n",
    "PREDICTION_FILE = \"pred.csv\"\n",
    "DO_DATA_AUGMENTATION = True\n",
    "\n",
    "\n",
    "SEED = 42 # reproductivity\n",
    "\n",
    "# if os.path.exists(TEST_FILE):\n",
    "#     print(f\"Loading training data from {TRAIN_FILE}...\")\n",
    "# else:\n",
    "#     raise FileNotFoundError(f\"Training file {TRAIN_FILE} not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400c6a51",
   "metadata": {},
   "source": [
    "## Labels, Tokenizer etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f18d6d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"no-hate\", 1: \"hate\"}\n",
    "label2id = {\"no-hate\": 0, \"hate\": 1}\n",
    "NUM_LABELS = len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dbb1bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "57bc5cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE_FINETUNE = (\n",
    "    \"Classify the following text as 'hate' or 'no-hate'.\\n\\n\"\n",
    "    \"Text: {text}\\n\"\n",
    "    \"Label: {label_str}{eos_token}\"\n",
    ")\n",
    "\n",
    "PROMPT_TEMPLATE_INFERENCE = (\n",
    "    \"Classify the following text as 'hate' or 'no-hate'.\\n\\n\"\n",
    "    \"Text: {text}\\n\"\n",
    "    \"Label:\"\n",
    ")\n",
    "\n",
    "# TODO prompta chyba lepiej po polsku dla polskich modeli? ale idk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f2a046",
   "metadata": {},
   "source": [
    "## Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "98120cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10041 training samples and 1000 test samples.\n",
      "                                            sentence  label\n",
      "0  Dla mnie faworytem do tytułu będzie Cracovia. ...      0\n",
      "1  @anonymized_account @anonymized_account Brawo ...      0\n",
      "2  @anonymized_account @anonymized_account Super,...      0\n",
      "3  @anonymized_account @anonymized_account Musi. ...      0\n",
      "4    Odrzut natychmiastowy, kwaśna mina, mam problem      0\n",
      "\n",
      "label\n",
      "0    9190\n",
      "1     851\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_train_full = pd.read_csv(TRAIN_FILE)\n",
    "    with open(TEST_FILE, 'r', encoding='utf-8') as f:\n",
    "        test_texts = [line.strip() for line in f]\n",
    "    df_test = pd.DataFrame(test_texts, columns=['sentence'])\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"err: no file {e.filename}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Loaded {len(df_train_full)} training samples and {len(df_test)} test samples.\")\n",
    "\n",
    "print(df_train_full.head())\n",
    "print()\n",
    "print(df_train_full['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ad559a",
   "metadata": {},
   "source": [
    "### Balancing classes (augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b002ad88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original distribution:\n",
      "label\n",
      "0    0.915247\n",
      "1    0.084753\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Original distribution:')\n",
    "print(df_train_full['label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5637bf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_majority = df_train_full[df_train_full['label'] == 0]\n",
    "df_minority = df_train_full[df_train_full['label'] == 1]\n",
    "\n",
    "df_minority_oversampled = df_minority.sample(\n",
    "    n=len(df_majority),\n",
    "    replace=True,\n",
    "    random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f38e39ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_balanced = pd.concat([df_majority, df_minority_oversampled])\n",
    "\n",
    "\n",
    "df_train_balanced = df_train_balanced.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "de6bc223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced distribution:\n",
      "label\n",
      "1    0.5\n",
      "0    0.5\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Balanced class nums \n",
      "label\n",
      "1    9190\n",
      "0    9190\n"
     ]
    }
   ],
   "source": [
    "print('Balanced distribution:')\n",
    "print(df_train_balanced['label'].value_counts(normalize=True))\n",
    "print(f\"\\nBalanced class nums \\n{df_train_balanced['label'].value_counts().to_string()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8042f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df_train_full = df_train_balanced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361aa13c",
   "metadata": {},
   "source": [
    "### Data augmentation v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "91b34a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO mozna zobaczyc czy to ma wiekszy sens zamiast tego powyzej^\n",
    "\n",
    "# if DO_DATA_AUGMENTATION:\n",
    "#     print(\"\\n--- Step 1a: Augmenting Data (Back-Translation) ---\")\n",
    "#     print(\"This may take a few minutes...\")\n",
    "#     try:\n",
    "#         # Initialize translation pipelines\n",
    "#         translator_pl_en = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-pl-en\", device=0 if torch.cuda.is_available() else -1)\n",
    "#         translator_en_pl = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-pl\", device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "#         def back_translate(text):\n",
    "#             try:\n",
    "#                 en_text = translator_pl_en(text, max_length=128)[0]['translation_text']\n",
    "#                 pl_text_augmented = translator_en_pl(en_text, max_length=128)[0]['translation_text']\n",
    "#                 return pl_text_augmented\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error during translation: {e}\")\n",
    "#                 return text  # Return original text on error\n",
    "\n",
    "#         # Augment the minority class (hate speech) to balance the dataset\n",
    "#         df_augmented_list = []\n",
    "#         # We assume the 'hate' class (1) is the minority\n",
    "#         texts_to_augment = df_train_full[df_train_full['label'] == 1]['text'].tolist()\n",
    "#         print(f\"Augmenting {len(texts_to_augment)} samples for the 'hate' class (label 1)\")\n",
    "#         for text in tqdm(texts_to_augment, desc=\"Augmenting 'hate' class\"):\n",
    "#             augmented_text = back_translate(text)\n",
    "#             if augmented_text != text:\n",
    "#                 df_augmented_list.append({'text': augmented_text, 'label': 1})\n",
    "\n",
    "#         if df_augmented_list:\n",
    "#             df_augmented = pd.DataFrame(df_augmented_list)\n",
    "#             df_train_full = pd.concat([df_train_full, df_augmented], ignore_index=True)\n",
    "#             print(\"\\nTraining set after augmentation:\")\n",
    "#             print(f\"New number of samples: {len(df_train_full)}\")\n",
    "#             print(df_train_full['label'].value_counts())\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Could not perform data augmentation: {e}. Continuing without it.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ec4141",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a7bae600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "669a23b6b2fe4b118d2402227c7ba395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15623 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e10c69751994beba153f5c4f1f50149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15623 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862f079e40904bbfb88ca62975a05e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15623 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ffec75c4b43496a891f3c2b835b5642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2757 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3239e84ef3e245d199e83f9760443fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2757 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53761bedcb784ca0a828df598dfc7ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2757 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Próbka danych po tokenizacji:\n",
      "{'input_ids': [9487, 1958, 262, 1708, 2420, 355, 705, 37035, 6, 393, 705, 3919, 12, 37035, 4458, 198, 198, 8206, 25, 2488, 272, 5177, 1143, 62, 23317, 573, 2188, 33721, 128, 247, 299, 444, 33320, 89, 769, 64, 41615, 368, 745, 13695, 494, 986, 1168, 707, 2101, 41615, 68, 129, 249, 25221, 1058, 35, 2488, 272, 5177, 1143, 62, 23317, 198, 33986, 25, 645, 12, 37035, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [9487, 1958, 262, 1708, 2420, 355, 705, 37035, 6, 393, 705, 3919, 12, 37035, 4458, 198, 198, 8206, 25, 2488, 272, 5177, 1143, 62, 23317, 573, 2188, 33721, 128, 247, 299, 444, 33320, 89, 769, 64, 41615, 368, 745, 13695, 494, 986, 1168, 707, 2101, 41615, 68, 129, 249, 25221, 1058, 35, 2488, 272, 5177, 1143, 62, 23317, 198, 33986, 25, 645, 12, 37035, 50256]}\n",
      "\n",
      "Zdekodowany tekst próbki:\n",
      "Classify the following text as 'hate' or 'no-hate'.\n",
      "\n",
      "Text: @anonymized_account tego się niespodziewałem po Tobie... Zawiodłeś nas :D @anonymized_account\n",
      "Label: no-hate<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(df_train_full, test_size=0.15, random_state=42, stratify=df_train_full['label'])\n",
    "\n",
    "raw_datasets = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df),\n",
    "    'validation': Dataset.from_pandas(val_df),\n",
    "    'test': Dataset.from_pandas(df_test)\n",
    "})\n",
    "\n",
    "def format_dataset_for_finetuning(examples):\n",
    "    texts = examples['sentence']\n",
    "    labels_int = examples['label']\n",
    "    formatted_prompts = []\n",
    "    for text, label_int in zip(texts, labels_int):\n",
    "        label_str = id2label[label_int]\n",
    "        formatted_prompts.append(\n",
    "            PROMPT_TEMPLATE_FINETUNE.format(\n",
    "                text=text,\n",
    "                label_str=label_str,\n",
    "                eos_token=tokenizer.eos_token\n",
    "            )\n",
    "        )\n",
    "    return {\"formatted_prompt\": formatted_prompts}\n",
    "\n",
    "def set_labels(examples):\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples\n",
    "\n",
    "tokenized_datasets = {}\n",
    "for split, data in raw_datasets.items():\n",
    "    if split in ['train', 'validation']: # not on test\n",
    "        formatted_data = data.map(format_dataset_for_finetuning, batched=True)\n",
    "        \n",
    "        tokenized_split = formatted_data.map(\n",
    "            lambda examples: tokenizer(\n",
    "                examples[\"formatted_prompt\"],\n",
    "                truncation=True,\n",
    "                max_length=256, # TODO potencjalnie zwiekszyz (zerknac jaka jest srednia dlugosc tekstu w danych + dlugosc promtu)\n",
    "                padding=False\n",
    "            ),\n",
    "            batched=True,\n",
    "            remove_columns=data.column_names + [\"formatted_prompt\"]\n",
    "        )\n",
    "        tokenized_datasets[split] = tokenized_split.map(set_labels, batched=True)\n",
    "\n",
    "print(\"\\nPróbka danych po tokenizacji:\")\n",
    "print(tokenized_datasets[\"train\"][0])\n",
    "print(\"\\nZdekodowany tekst próbki:\")\n",
    "print(tokenizer.decode(tokenized_datasets[\"train\"][0]['input_ids']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede76610",
   "metadata": {},
   "source": [
    "## Config LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "738f1637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bq_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# ) \n",
    "# # TODO mialem err z  BitsAndBytesConfig\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    # quantization_config=bq_config,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=False\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e3897fdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'bitsandbytes' has no attribute 'nn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msubprocess\u001b[39;00m\n\u001b[32m      2\u001b[39m lora_config = LoraConfig(\n\u001b[32m      3\u001b[39m     r=\u001b[32m16\u001b[39m,\n\u001b[32m      4\u001b[39m     lora_alpha=\u001b[32m32\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     task_type=TaskType.CAUSAL_LM\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m model = \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m model.print_trainable_parameters()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM/.venv/lib/python3.13/site-packages/peft/mapping_func.py:123\u001b[39m, in \u001b[36mget_peft_model\u001b[39m\u001b[34m(model, peft_config, adapter_name, mixed, autocast_adapter_dtype, revision, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m peft_config.is_prompt_learning:\n\u001b[32m    122\u001b[39m     peft_config = _prepare_prompt_learning_config(peft_config, model_config)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM/.venv/lib/python3.13/site-packages/peft/peft_model.py:1722\u001b[39m, in \u001b[36mPeftModelForCausalLM.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, **kwargs)\u001b[39m\n\u001b[32m   1719\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m   1720\u001b[39m     \u001b[38;5;28mself\u001b[39m, model: torch.nn.Module, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m, **kwargs\n\u001b[32m   1721\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1722\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1723\u001b[39m     \u001b[38;5;28mself\u001b[39m.base_model_prepare_inputs_for_generation = \u001b[38;5;28mself\u001b[39m.base_model.prepare_inputs_for_generation\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM/.venv/lib/python3.13/site-packages/peft/peft_model.py:132\u001b[39m, in \u001b[36mPeftModel.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    130\u001b[39m     ctx = init_empty_weights \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx():\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m         \u001b[38;5;28mself\u001b[39m.base_model = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[38;5;28mself\u001b[39m.set_additional_trainable_modules(peft_config, adapter_name)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.base_model, \u001b[33m\"\u001b[39m\u001b[33m_cast_adapter_dtype\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM/.venv/lib/python3.13/site-packages/peft/tuners/lora/model.py:142\u001b[39m, in \u001b[36mLoraModel.__init__\u001b[39m\u001b[34m(self, model, config, adapter_name, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name, low_cpu_mem_usage: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM/.venv/lib/python3.13/site-packages/peft/tuners/tuners_utils.py:180\u001b[39m, in \u001b[36mBaseTuner.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28mself\u001b[39m._pre_injection_hook(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.peft_config[adapter_name], adapter_name)\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m peft_config != PeftType.XLORA \u001b[38;5;129;01mor\u001b[39;00m peft_config[adapter_name] != PeftType.XLORA:\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[38;5;28mself\u001b[39m.model.peft_config = \u001b[38;5;28mself\u001b[39m.peft_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM/.venv/lib/python3.13/site-packages/peft/tuners/tuners_utils.py:508\u001b[39m, in \u001b[36mBaseTuner.inject_adapter\u001b[39m\u001b[34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    506\u001b[39m         ctx = init_empty_weights \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[32m    507\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m ctx():\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_and_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.targeted_module_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m uses_dummy_target_modules:\n\u001b[32m    511\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m excluded_modules \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unmatched_modules:\n\u001b[32m    512\u001b[39m         \u001b[38;5;66;03m# All targeted modules were excluded\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM/.venv/lib/python3.13/site-packages/peft/tuners/lora/model.py:237\u001b[39m, in \u001b[36mLoraModel._create_and_replace\u001b[39m\u001b[34m(self, lora_config, adapter_name, target, target_name, parent, current_key)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    236\u001b[39m     device_map = \u001b[38;5;28mself\u001b[39m.model.hf_device_map \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mhf_device_map\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     new_module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m adapter_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.active_adapters:\n\u001b[32m    239\u001b[39m         \u001b[38;5;66;03m# adding an additional adapter: it is not automatically trainable\u001b[39;00m\n\u001b[32m    240\u001b[39m         new_module.requires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM/.venv/lib/python3.13/site-packages/peft/tuners/lora/model.py:318\u001b[39m, in \u001b[36mLoraModel._create_new_module\u001b[39m\u001b[34m(lora_config, adapter_name, target, **kwargs)\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;66;03m# avoid eager bnb import\u001b[39;00m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_bnb_available():\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbnb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dispatch_bnb_8bit\n\u001b[32m    320\u001b[39m     dispatchers.append(dispatch_bnb_8bit)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_bnb_4bit_available():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM/.venv/lib/python3.13/site-packages/peft/tuners/lora/bnb.py:296\u001b[39m\n\u001b[32m    291\u001b[39m             new_module = Linear8bitLt(target, adapter_name, **eightbit_kwargs)\n\u001b[32m    293\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m new_module\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_bnb_4bit_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mLinear4bit\u001b[39;00m(torch.nn.Module, LoraLayer):\n\u001b[32m    299\u001b[39m         \u001b[38;5;66;03m# Lora implemented in a dense layer\u001b[39;00m\n\u001b[32m    300\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    301\u001b[39m             \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    302\u001b[39m             base_layer: torch.nn.Module,\n\u001b[32m   (...)\u001b[39m\u001b[32m    311\u001b[39m             **kwargs,\n\u001b[32m    312\u001b[39m         ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM/.venv/lib/python3.13/site-packages/peft/import_utils.py:35\u001b[39m, in \u001b[36mis_bnb_4bit_available\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbitsandbytes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbnb\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[43mbnb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mLinear4bit\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'bitsandbytes' has no attribute 'nn'"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"c_proj\", \"c_attn\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef8d9f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'bitsandbytes' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbitsandbytes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbnb\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mbnb\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__version__\u001b[49m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'bitsandbytes' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "print(bnb.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
