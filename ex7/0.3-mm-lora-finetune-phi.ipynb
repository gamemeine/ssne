{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00db5f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 11 20:44:55 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A10                     On  |   00000000:06:00.0 Off |                    0 |\n",
      "|  0%   46C    P0             59W /  150W |    6309MiB /  23028MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A           33610      C   ...untu/ssne-mm/.venv/bin/python       6300MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43035201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ssne-mm/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import bitsandbytes as bnb\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa21c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"microsoft/phi-1_5\"\n",
    "LORA_MODEL_OUTPUT_DIR = \"./hate-speech-lora-model\"\n",
    "TRAIN_FILE = \"data/hate_train.csv\"\n",
    "TEST_FILE = \"data/hate_test_data.txt\"\n",
    "PREDICTION_FILE = \"pred.csv\"\n",
    "\n",
    "SEED = 42 # reproductivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52d3b744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10041 training samples and  test samples.\n",
      "                                            sentence  label\n",
      "0  Dla mnie faworytem do tytułu będzie Cracovia. ...      0\n",
      "1  @anonymized_account @anonymized_account Brawo ...      0\n",
      "2  @anonymized_account @anonymized_account Super,...      0\n",
      "3  @anonymized_account @anonymized_account Musi. ...      0\n",
      "4    Odrzut natychmiastowy, kwaśna mina, mam problem      0\n",
      "\n",
      "label\n",
      "0    9190\n",
      "1     851\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train_full = pd.read_csv(TRAIN_FILE)\n",
    "with open(TEST_FILE, 'r', encoding='utf-8') as f:\n",
    "    test_texts = [line.strip() for line in f]\n",
    "df_test = pd.DataFrame(test_texts, columns=['sentence'])\n",
    "\n",
    "print(f\"Loaded {len(df_train_full)} training samples and  test samples.\")\n",
    "\n",
    "print(df_train_full.head())\n",
    "print()\n",
    "print(df_train_full['label'].value_counts())\n",
    "\n",
    "\n",
    "train_df, val_df = train_test_split(df_train_full, test_size=0.15, random_state=42, stratify=df_train_full['label'])\n",
    "\n",
    "\n",
    "raw_datasets = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df),\n",
    "    'validation': Dataset.from_pandas(val_df),\n",
    "    'test': Dataset.from_pandas(df_test)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f72fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "# )\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae8d3ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,291,456 || all params: 1,424,562,176 || trainable%: 0.4416\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"dense\",\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84408650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 8534/8534 [00:00<00:00, 35782.17 examples/s]\n",
      "Map: 100%|██████████| 8534/8534 [00:02<00:00, 3494.36 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 1507/1507 [00:00<00:00, 10389.94 examples/s]\n",
      "Map: 100%|██████████| 1507/1507 [00:00<00:00, 3606.85 examples/s]\n",
      "Map: 100%|██████████| 8534/8534 [00:01<00:00, 5860.89 examples/s]\n",
      "Map: 100%|██████████| 1507/1507 [00:00<00:00, 6053.74 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Próbka danych po tokenizacji z nowym promptem:\n",
      "Przeanalizuj poniższy tekst i określ, czy jest to mowa nienawiści. Odpowiedz '1' dla mowy nienawiści lub '0' w przeciwnym razie. Oto przykłady:\n",
      "\n",
      "Tekst: Dziękuję za miłą rozmowę, to był bardzo produktywny dzień.\n",
      "Odpowiedź: 0\n",
      "\n",
      "Tekst: Wszyscy imigranci powinni stąd zniknąć, psują nasz kraj.\n",
      "Odpowiedź: 1\n",
      "\n",
      "Tekst: @anonymized_account Najgorzej jak polityk mieniący się konserwatystą nie odróżnia konserwatysty od konserwy.\n",
      "Odpowiedź: 0<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_NON_HATE = \"Dziękuję za miłą rozmowę, to był bardzo produktywny dzień.\"\n",
    "EXAMPLE_HATE = \"Wszyscy imigranci powinni stąd zniknąć, psują nasz kraj.\"\n",
    "\n",
    "PROMPT_TEMPLATE_FINETUNE = (\n",
    "    \"Przeanalizuj poniższy tekst i określ, czy jest to mowa nienawiści. \"\n",
    "    \"Odpowiedz '1' dla mowy nienawiści lub '0' w przeciwnym razie. \"\n",
    "    \"Oto przykłady:\\n\\n\"\n",
    "    \"Tekst: {example_non_hate_text}\\n\"\n",
    "    \"Odpowiedź: 0\\n\\n\"\n",
    "    \"Tekst: {example_hate_text}\\n\"\n",
    "    \"Odpowiedź: 1\\n\\n\"\n",
    "    \"Tekst: {text}\\n\"\n",
    "    \"Odpowiedź: {label_str}{eos_token}\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def format_dataset_for_finetuning(examples):\n",
    "    texts = examples['sentence']\n",
    "    labels_int = examples['label']\n",
    "    formatted_prompts = []\n",
    "    for text, label_int in zip(texts, labels_int):\n",
    "        label_str = str(label_int)\n",
    "        formatted_prompts.append(\n",
    "            PROMPT_TEMPLATE_FINETUNE.format(\n",
    "                example_non_hate_text=EXAMPLE_NON_HATE,\n",
    "                example_hate_text=EXAMPLE_HATE,\n",
    "                text=text,\n",
    "                label_str=label_str,\n",
    "                eos_token=tokenizer.eos_token\n",
    "            )\n",
    "        )\n",
    "    return {\"formatted_prompt\": formatted_prompts}\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"formatted_prompt\"],\n",
    "        truncation=True,\n",
    "        max_length=312,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized_datasets = DatasetDict()\n",
    "\n",
    "for split in ['train', 'validation']:\n",
    "    formatted_data = raw_datasets[split].map(format_dataset_for_finetuning, batched=True, num_proc=4)\n",
    "    tokenized_datasets[split] = formatted_data.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=raw_datasets[split].column_names + [\"formatted_prompt\"]\n",
    "    )\n",
    "\n",
    "def set_labels(examples):\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples\n",
    "\n",
    "tokenized_datasets['train'] = tokenized_datasets['train'].map(set_labels, batched=True)\n",
    "tokenized_datasets['validation'] = tokenized_datasets['validation'].map(set_labels, batched=True)\n",
    "\n",
    "\n",
    "print(\"\\nPróbka danych po tokenizacji z nowym promptem:\")\n",
    "print(tokenizer.decode(tokenized_datasets[\"train\"][0]['input_ids'], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2782658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_49394/689565100.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=LORA_MODEL_OUTPUT_DIR,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"hate-speech-lora-finetuning\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a2db8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 11 20:45:08 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A10                     On  |   00000000:06:00.0 Off |                    0 |\n",
      "|  0%   45C    P0             59W /  150W |    8194MiB /  23028MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A           33610      C   ...untu/ssne-mm/.venv/bin/python       6300MiB |\n",
      "|    0   N/A  N/A           49394      C   ...untu/ssne-mm/.venv/bin/python       1880MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8870d447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rozpoczynanie treningu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m01178563\u001b[0m (\u001b[33m01178563-warsaw-information-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/ssne-mm/ssne/ex7/wandb/run-20250611_204508-5nbuwu00</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/01178563-warsaw-information-technology/huggingface/runs/5nbuwu00' target=\"_blank\">hate-speech-lora-finetuning</a></strong> to <a href='https://wandb.ai/01178563-warsaw-information-technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/01178563-warsaw-information-technology/huggingface' target=\"_blank\">https://wandb.ai/01178563-warsaw-information-technology/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/01178563-warsaw-information-technology/huggingface/runs/5nbuwu00' target=\"_blank\">https://wandb.ai/01178563-warsaw-information-technology/huggingface/runs/5nbuwu00</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ssne-mm/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='1067' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  50/1067 01:06 < 23:24, 0.72 it/s, Epoch 0.05/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nRozpoczynanie treningu...\")\n",
    "trainer.train()\n",
    "\n",
    "best_model_path = LORA_MODEL_OUTPUT_DIR+ \"/best\"\n",
    "trainer.save_model(best_model_path)\n",
    "print(f\"Najlepszy model (adaptery LoRA) zapisany w: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6d41d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE_INFERENCE = (\n",
    "    \"Przeanalizuj poniższy tekst i określ, czy jest to mowa nienawiści. \"\n",
    "    \"Odpowiedz '1' dla mowy nienawiści lub '0' w przeciwnym razie. \"\n",
    "    \"Oto przykłady:\\n\\n\"\n",
    "    \"Tekst: {example_non_hate_text}\\n\"\n",
    "    \"Odpowiedź: 0\\n\\n\"\n",
    "    \"Tekst: {example_hate_text}\\n\"\n",
    "    \"Odpowiedź: 1\\n\\n\"\n",
    "    \"Tekst: {text}\\n\"\n",
    "    \"Odpowiedź: \"\n",
    ")\n",
    "\n",
    "\n",
    "base_model_for_inference = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "inference_model = PeftModel.from_pretrained(base_model_for_inference, best_model_path)\n",
    "print(f\"Załadowano model z adapterami z: {best_model_path}\")\n",
    "\n",
    "inference_model.eval()\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "\n",
    "for sample in tqdm(raw_datasets['validation'], desc=\"Ewaluacja na zbiorze walidacyjnym\"):\n",
    "    text = sample['sentence']\n",
    "    true_label = sample['label']\n",
    "\n",
    "    y_true.append(true_label)\n",
    "\n",
    "    prompt = PROMPT_TEMPLATE_INFERENCE.format(\n",
    "        example_non_hate_text=EXAMPLE_NON_HATE,\n",
    "        example_hate_text=EXAMPLE_HATE,\n",
    "        text=text\n",
    "    )    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generate_ids = inference_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=3,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    output_text = tokenizer.decode(generate_ids[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "    if output_text in ['0', '1']:\n",
    "        prediction = int(output_text)\n",
    "        print(f\"Output: '{output_text}' | Prawdziwa etykieta: {true_label} | Przewidywana etykieta: {prediction} | Tekst: '{text}' \")\n",
    "    else:\n",
    "        print(f\"Niepoprawna odpowiedź modelu: '{output_text}' dla tekstu: '{text}'\")\n",
    "        prediction = 1 - true_label\n",
    "\n",
    "    y_pred.append(prediction)\n",
    "\n",
    "print(\"\\n--- Wyniki ewaluacji na zbiorze walidacyjnym ---\")\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Dokładność (Accuracy): {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nRaport klasyfikacji:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Not Hate-Speech (0)', 'Hate-Speech (1)']))\n",
    "\n",
    "print(\"\\nMacierz pomyłek:\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=['Not Hate-Speech (0)', 'Hate-Speech (1)'],\n",
    "    yticklabels=['Not Hate-Speech (0)', 'Hate-Speech (1)']\n",
    ")\n",
    "plt.ylabel('Prawdziwa etykieta (True Label)')\n",
    "plt.xlabel('Przewidziana etykieta (Predicted Label)')\n",
    "plt.title('Macierz pomyłek')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
