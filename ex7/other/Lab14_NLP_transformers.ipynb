{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced NLP Models Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    CLIPModel,\n",
        "    CLIPProcessor,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TEXT CLASSIFICATION AND SENTIMENT ANALYSIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://huggingface.co/docs/transformers/en/main_classes/pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentiment_pipeline = pipeline(\n",
        "    \"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        ")\n",
        "\n",
        "texts = [\n",
        "    \"I love this new transformer model!\",\n",
        "    \"This is terrible, I hate it.\",\n",
        "    \"The weather is okay today, nothing special.\",\n",
        "    \"Machine learning is revolutionizing the world!\",\n",
        "]\n",
        "\n",
        "for text in texts:\n",
        "    result = sentiment_pipeline(text)\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Sentiment: {result[0]['label']} (confidence: {result[0]['score']:.3f})\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CUSTOM TEXT CLASSIFICATION WITH TRANSFER LEARNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomTextClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name, num_labels=2, output_attentions=False, output_hidden_states=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample data (in practice, you'd load your own dataset)\n",
        "sample_texts = [\n",
        "    \"This product is amazing and works perfectly!\",\n",
        "    \"Terrible quality, broke after one day.\",\n",
        "    \"Great value for money, highly recommend.\",\n",
        "    \"Worst purchase ever, complete waste of money.\",\n",
        "]\n",
        "sample_labels = [1, 0, 1, 0]  # 1: positive, 0: negative\n",
        "\n",
        "# Create dataset\n",
        "dataset = CustomTextClassificationDataset(sample_texts, sample_labels, tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print number of trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Number of trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=20,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=dataset,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the trained model\n",
        "print(\"\\n=== Evaluating Trained Model ===\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Create evaluation dataset with 4 samples\n",
        "eval_texts = [\n",
        "    \"The service was excellent and very professional\",\n",
        "    \"I'm disappointed with the quality of this product\",\n",
        "    \"This exceeded my expectations, great purchase!\",\n",
        "    \"Not worth the money, poor performance\",\n",
        "    \"Awful awful service, I'm never coming back\",\n",
        "]\n",
        "eval_labels = [1, 0, 1, 0, 0]  # 1: positive, 0: negative\n",
        "\n",
        "# Create evaluation dataset\n",
        "eval_dataset = CustomTextClassificationDataset(eval_texts, eval_labels, tokenizer)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Create evaluation dataloader\n",
        "eval_dataloader = torch.utils.data.DataLoader(eval_dataset, batch_size=2, shuffle=False)\n",
        "\n",
        "# Lists to store predictions and true labels\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "# Evaluate without gradient computation\n",
        "with torch.no_grad():\n",
        "    for batch in eval_dataloader:\n",
        "        # Move batch to device\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Get model predictions\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "        # Store predictions and labels\n",
        "        all_predictions.extend(predictions.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print detailed results\n",
        "print(\"\\nDetailed Results:\")\n",
        "for i, (text, true_label, pred) in enumerate(\n",
        "    zip(sample_texts, all_labels, all_predictions)\n",
        "):\n",
        "    print(f\"\\nSample {i + 1}:\")\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"True Label: {'Positive' if true_label == 1 else 'Negative'}\")\n",
        "    print(f\"Predicted: {'Positive' if pred == 1 else 'Negative'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CUSTOM TRAINING LOOP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name, num_labels=2, output_attentions=False, output_hidden_states=False\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 20\n",
        "batch_size = 8\n",
        "learning_rate = 2e-4\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=batch_size, shuffle=True\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        # Move batch to device\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(\n",
        "            input_ids=input_ids, attention_mask=attention_mask, labels=labels\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LORA TRAINING LOOP\n",
        "https://huggingface.co/docs/peft/en/index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name, num_labels=2, output_attentions=False, output_hidden_states=False\n",
        ")\n",
        "model.to(device)\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # rank\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_lin\", \"v_lin\"],  # target attention layers\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\",\n",
        ")\n",
        "\n",
        "# Prepare model for LoRA fine-tuning\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "print(model)\n",
        "\n",
        "# Configure training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora_results\",\n",
        "    num_train_epochs=20,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    logging_dir=\"./lora_logs\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,  # Enable mixed precision training\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=dataset,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print number of trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Number of trainable parameters: {trainable_params:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## QUESTION ANSWERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qa_pipeline = pipeline(\n",
        "    \"question-answering\", model=\"distilbert-base-cased-distilled-squad\"\n",
        ")\n",
        "\n",
        "context = \"\"\"\n",
        "The transformer architecture was introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017.\n",
        "It revolutionized natural language processing by using self-attention mechanisms instead of recurrent layers.\n",
        "The model consists of an encoder and decoder, each made up of multiple layers with multi-head attention and\n",
        "feed-forward networks. BERT, GPT, and T5 are all based on the transformer architecture.\n",
        "\"\"\"\n",
        "\n",
        "questions = [\n",
        "    \"When was the transformer architecture introduced?\",\n",
        "    \"What did the transformer architecture replace?\",\n",
        "    \"What are some models based on transformer architecture?\",\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    answer = qa_pipeline(question=question, context=context)\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {answer['answer']} (confidence: {answer['score']:.3f})\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CLIP\n",
        "\n",
        "https://openai.com/index/clip/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Load CLIP model\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Load and process the cat image\n",
        "image = Image.open(\"cat.jpg\")\n",
        "image_inputs = clip_processor(images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "# Text descriptions to compare with the image\n",
        "text_descriptions = [\n",
        "    \"a photo of a cat\",\n",
        "    \"a photo of a dog\",\n",
        "    \"a photo of a car\",\n",
        "    \"a photo of a bicycle\",\n",
        "    \"a photo of a bird\",\n",
        "]\n",
        "\n",
        "# Get image and text features\n",
        "image_features = clip_model.get_image_features(**image_inputs)\n",
        "text_inputs = clip_processor(text=text_descriptions, return_tensors=\"pt\", padding=True)\n",
        "text_features = clip_model.get_text_features(**text_inputs)\n",
        "\n",
        "# Compute similarity between image and text descriptions\n",
        "similarity_scores = torch.cosine_similarity(image_features, text_features, dim=1)\n",
        "\n",
        "# Print results\n",
        "print(\"\\nSimilarity scores between cat image and text descriptions:\")\n",
        "for text, score in zip(text_descriptions, similarity_scores):\n",
        "    print(f\"{text}: {score.item():.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
