{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02dec845",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Grzegorz Statkiewicz, Mateusz Matukiewicz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c644fa77",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The structure of the direcotry should be as follows:\n",
    "\n",
    "```\n",
    ".\n",
    "├── data\n",
    "│   ├── train.pkl\n",
    "│   └── test_no_target.pkl\n",
    "└── main.ipynb\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5290e644",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92cc3a2",
   "metadata": {},
   "source": [
    "Select the device to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c26c9c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 25 12:55:36 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.02              Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1660 Ti     On  |   00000000:1C:00.0  On |                  N/A |\n",
      "|  0%   46C    P8             17W /  130W |     993MiB /   6144MiB |      3%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A       377      G   /Xwayland                                   N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0b8098fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "746dc9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebff3e06",
   "metadata": {},
   "source": [
    "### Config for reproductivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c99387ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05f8042",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7b2553",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fd66a60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"data/train.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3ab81607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"Loads data from a pickle file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6a560b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2939 training samples.\n",
      "(array([ -1.,  -1.,  -1., ...,  78.,  40., 144.], shape=(4756,)), 0)\n"
     ]
    }
   ],
   "source": [
    "train_data = load_data(train_path)\n",
    "\n",
    "print(f\"Loaded {len(train_data)} training samples.\")\n",
    "\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b0a373d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compositors = {0: 'bach', 1: 'beethoven', 2: 'debussy', 3: 'scarlatti', 4: 'victoria'}\n",
    "num_classes = len(compositors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c2d8c8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 193\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sequences = [torch.tensor(seq, dtype=torch.long) for (seq, label) in train_data]\n",
    "labels = [label for (seq, label) in train_data]\n",
    "\n",
    "# Find the max chord index (vocab size, since chords are ints)\n",
    "all_chords = set()\n",
    "for seq in sequences:\n",
    "    all_chords.update(seq.tolist())\n",
    "vocab_size = int(max(all_chords)) + 2  # +1 for max, +1 for padding idx=0\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "408de834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class ChordDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = [seq + 1 for seq in sequences]\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    seqs, labels = zip(*batch)\n",
    "    lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    padded_seqs = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "    return padded_seqs, lengths, torch.tensor(labels, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "61595462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_data_split, val_data_split = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_sequences = [torch.tensor(seq, dtype=torch.long) for (seq, label) in train_data_split]\n",
    "train_labels = [label for (seq, label) in train_data_split]\n",
    "val_sequences = [torch.tensor(seq, dtype=torch.long) for (seq, label) in val_data_split]\n",
    "val_labels = [label for (seq, label) in val_data_split]\n",
    "\n",
    "train_dataset = ChordDataset(train_sequences, train_labels)\n",
    "val_dataset = ChordDataset(val_sequences, val_labels)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1e772fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "class_sample_counts = np.array([train_labels.count(i) for i in range(num_classes)])\n",
    "weights = 1. / class_sample_counts\n",
    "\n",
    "sample_weights = np.array([weights[label] for label in train_labels])\n",
    "sample_weights = torch.DoubleTensor(sample_weights)\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "sampled_train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649c4f44",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c0031c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, num_layers, dropout_p=0.5, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0) # padding_idx=0 assumes 0 is used for padding\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,  \n",
    "            dropout=dropout_p if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc = nn.Linear(fc_input_dim, output_dim)\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "\n",
    "\n",
    "        # --- START DEBUGGING EMBEDDING INPUT ---\n",
    "        print(f\"--- Inside LSTMClassifier.forward ---\")\n",
    "        print(f\"Input x device: {x.device}\")\n",
    "        print(f\"Input x dtype: {x.dtype}\")\n",
    "        print(f\"Input x shape: {x.shape}\")\n",
    "        if x.numel() > 0: # Check if tensor is not empty\n",
    "            print(f\"Min value in x: {x.min().item()}\")\n",
    "            print(f\"Max value in x: {x.max().item()}\")\n",
    "        else:\n",
    "            print(\"Input x is empty!\")\n",
    "        print(f\"Embedding layer vocab size (num_embeddings): {self.embedding.num_embeddings}\")\n",
    "        print(f\"Embedding layer padding_idx: {self.embedding.padding_idx}\")\n",
    "\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        # LSTM\n",
    "        packed_output, (h_n, c_n) = self.lstm(packed_embedded)\n",
    "\n",
    "    \n",
    "        if self.bidirectional:\n",
    "            h_n_last_layer_forward = h_n[-2, :, :]\n",
    "            h_n_last_layer_backward = h_n[-1, :, :]\n",
    "            hidden = torch.cat((h_n_last_layer_forward, h_n_last_layer_backward), dim=1)\n",
    "        else:\n",
    "            hidden = h_n[-1, :, :]\n",
    "\n",
    "\n",
    "        hidden = self.dropout(hidden)\n",
    "        logits = self.fc(hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05700535",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = vocab_size\n",
    "EMBED_DIM = 32\n",
    "HIDDEN_DIM = 64\n",
    "OUTPUT_DIM = 5\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT_P = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "25d5b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        output_dim=OUTPUT_DIM,\n",
    "        num_layers=2,\n",
    "        dropout_p=DROPOUT_P,\n",
    "        bidirectional=False\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ad22a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "248571d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMClassifier(\n",
      "  (embedding): Embedding(193, 32, padding_idx=0)\n",
      "  (lstm): LSTM(32, 64, num_layers=2, batch_first=True, dropout=0.4)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (fc): Linear(in_features=64, out_features=5, bias=True)\n",
      ")\n",
      "Number of parameters: 64869\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393c3a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "  Train Loss: 1.6037, Train Acc: 0.2144\n",
      "  Val Loss: 1.5910,   Val Acc: 0.1633\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(sequences_batch, lengths_batch)\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels_batch)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     19\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m sequences_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# loss.item() is avg loss for batch\u001b[39;00m\n",
      "File \u001b[0;32m~/SSNE/.venv/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SSNE/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SSNE/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for i, (sequences_batch, lengths_batch, labels_batch) in enumerate(sampled_train_loader):\n",
    "        sequences_batch = sequences_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(sequences_batch, lengths_batch)\n",
    "        loss = criterion(outputs, labels_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * sequences_batch.size(0)\n",
    "\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted_labels == labels_batch).sum().item()\n",
    "        total_predictions += labels_batch.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total_predictions\n",
    "    epoch_acc = correct_predictions / total_predictions\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for sequences_batch, lengths_batch, labels_batch in val_loader:\n",
    "            sequences_batch = sequences_batch.to(device)\n",
    "            labels_batch = labels_batch.to(device)\n",
    "\n",
    "            outputs = model(sequences_batch, lengths_batch)\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "            val_running_loss += loss.item() * sequences_batch.size(0)\n",
    "\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "            val_correct_predictions += (predicted_labels == labels_batch).sum().item()\n",
    "            val_total_predictions += labels_batch.size(0)\n",
    "\n",
    "    val_epoch_loss = val_running_loss / val_total_predictions\n",
    "    val_epoch_acc = val_correct_predictions / val_total_predictions\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
    "    print(f\"  Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_epoch_loss:.4f},   Val Acc: {val_epoch_acc:.4f}\")\n",
    "\n",
    "# print(\"Training finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
