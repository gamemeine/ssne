{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")  # torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(73512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dane syntetyczne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(0, 50, 0.02)\n",
    "synthetic_data = (\n",
    "    np.sin(indices * 3) + indices / 10 + (indices / 10) ** 2 + np.sin(indices * 10)\n",
    ")  # / np.exp(indices / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20, 10])\n",
    "plt.plot(synthetic_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = synthetic_data.min()\n",
    "max_value = synthetic_data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_seq = []\n",
    "data_targets = []\n",
    "sequence_len = 150\n",
    "for i in range(len(synthetic_data) - sequence_len - 1):\n",
    "    data_seq.append(torch.from_numpy(synthetic_data[i : i + sequence_len]))\n",
    "    data_targets.append(synthetic_data[i + sequence_len + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (torch.stack(data_seq).float() - min_value) / max_value\n",
    "data_targets = (torch.Tensor(data_targets).float() - min_value) / max_value\n",
    "train_indices = rng.random(len(data_seq)) > 0.3\n",
    "test_indices = ~train_indices\n",
    "train_set = torch.utils.data.TensorDataset(\n",
    "    data[train_indices], data_targets[train_indices]\n",
    ")\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_data, test_targets = data[test_indices], data_targets[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRegressor(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.act_fn = nn.Tanh()\n",
    "        self.linear2 = nn.Linear(num_hidden, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = SimpleRegressor(sequence_len, 5, 1).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fun = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(101):\n",
    "    for x, targets in train_loader:\n",
    "        x = x.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        preds = model(x)\n",
    "        preds = preds.squeeze(dim=1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fun(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}, loss: {loss.item():.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_preds = model(test_data.to(device))\n",
    "    print(torch.abs(test_preds.squeeze() - test_targets.to(device)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 5])\n",
    "plt.plot(test_targets.numpy(), label=\"original\")\n",
    "plt.plot(test_preds.cpu().numpy(), label=\"predictions\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co jest nie tak?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(len(data) * 0.7)\n",
    "train_set = torch.utils.data.TensorDataset(\n",
    "    data[:train_split], data_targets[:train_split]\n",
    ")\n",
    "train_loader = DataLoader(train_set, batch_size=32, drop_last=True)\n",
    "test_data, test_targets = data[train_split:], data_targets[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRegressor(sequence_len, 5, 1).to(device)\n",
    "model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fun = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(101):\n",
    "    for x, targets in train_loader:\n",
    "        x = x.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        preds = model(x)\n",
    "        preds = preds.squeeze(dim=1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fun(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}, loss: {loss.item():.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_preds = model(test_data.to(device))\n",
    "    print(torch.abs(test_preds.squeeze() - test_targets.to(device)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 5])\n",
    "plt.plot(test_targets.numpy(), label=\"original\")\n",
    "plt.plot(test_preds.cpu().numpy(), label=\"predictions_short_term\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sieć rekurencyjna\n",
    "![](https://cdn.jsdelivr.net/gh/AuthurWhywait/images/20211202145041.png)\n",
    "![](https://cdn.jsdelivr.net/gh/AuthurWhywait/images/20211202151931.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentRegressor(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
    "        super().__init__()\n",
    "        self.act_fn = nn.Tanh()\n",
    "        self.linear_xh = nn.Linear(num_inputs, num_hidden)\n",
    "        self.linear_hh = nn.Linear(num_hidden, num_hidden)\n",
    "        self.linear_hy = nn.Linear(num_hidden, num_outputs)\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        h = self.act_fn(self.linear_hh(state) + self.linear_xh(x))\n",
    "        y = self.linear_hy(h)\n",
    "        return y, h\n",
    "\n",
    "\n",
    "HIDDEN_SIZE = 5\n",
    "model = RecurrentRegressor(1, HIDDEN_SIZE, 1).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dokończ pętlę uczącą poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fun = nn.MSELoss()\n",
    "\n",
    "for epoch in range(101):\n",
    "    for x, targets in train_loader:\n",
    "        x = x.to(device)\n",
    "        targets = targets.to(device)\n",
    "        loss_total = 0\n",
    "        state = torch.zeros(len(x), HIDDEN_SIZE).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        for i in range(x.size(1)):\n",
    "            x_one = x[:, i].unsqueeze(1)\n",
    "            if i < sequence_len - 1:\n",
    "                target = x[:, i + 1]\n",
    "            else:\n",
    "                target = targets\n",
    "            preds, state = model(x_one, state)\n",
    "            preds = preds.squeeze(dim=1)\n",
    "            loss = loss_fun(preds, target)\n",
    "            loss_total += loss\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}, loss: {loss.item():.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    state = torch.zeros(len(test_data), HIDDEN_SIZE).to(device)\n",
    "    test_preds = []\n",
    "    for i in range(test_data.size(1)):\n",
    "        x_one = test_data[:, i].unsqueeze(1).to(device)\n",
    "        preds, state = model(x_one, state)\n",
    "    test_preds.append(preds)\n",
    "    print(torch.abs((torch.cat(test_preds).squeeze() - test_targets.to(device))).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 5])\n",
    "plt.plot(test_targets, label=\"original\")\n",
    "plt.plot(torch.cat(test_preds).squeeze().cpu().numpy(), label=\"predictions\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sieci rekurencyjne w Torchu\n",
    "\n",
    "https://docs.pytorch.org/docs/stable/generated/torch.nn.RNN.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "INPUT_SIZE = 3\n",
    "NUM_LAYERS = 1\n",
    "rnn = nn.RNN(\n",
    "    input_size=INPUT_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    batch_first=False,\n",
    ")  # batch_first=False is default!!\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sequence_len = 5\n",
    "    x = torch.randn(sequence_len, BATCH_SIZE, INPUT_SIZE)\n",
    "    h0 = torch.randn(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE)\n",
    "    output, hn = rnn(x, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape  # seq_len, batch_size, hidden_size\n",
    "# output features (h_t) from the last layer of the RNN, for each t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hn.shape  # num_layers, batch_size, hidden_size\n",
    "# the final hidden state for each element in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, out_size):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size, hidden_size=hidden_size, num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, out_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "        all_outputs, hidden = self.rnn(x, hidden)\n",
    "        out = all_outputs[-1]  # We are interested only in the last output\n",
    "        x = self.fc(out)\n",
    "        return x, hidden\n",
    "\n",
    "\n",
    "model = RNNRegressor(1, HIDDEN_SIZE, NUM_LAYERS, 1).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fun = nn.MSELoss()\n",
    "\n",
    "for epoch in range(101):\n",
    "    for x, targets in train_loader:\n",
    "        x = x.to(device).unsqueeze(2)\n",
    "        targets = targets.to(device)\n",
    "        hidden = model.init_hidden(x.size(0)).to(device)\n",
    "        preds, last_hidden = model(x, hidden)\n",
    "        preds = preds.squeeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fun(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}, loss: {loss.item():.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    hidden = model.init_hidden(len(test_data)).to(device)\n",
    "    test_preds, _ = model(test_data.to(device).unsqueeze(2), hidden)\n",
    "    print(torch.abs(test_preds.squeeze() - test_targets.to(device)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 5])\n",
    "plt.plot(test_targets, label=\"original\")\n",
    "plt.plot(test_preds.squeeze().cpu().numpy(), label=\"predictions\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Shor Term Memory Networks (LSTM): Czy możemy jakoś rozdzielić krótką i długą pamięć?\n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM](https://cdn-images-1.medium.com/max/1000/1*Ht2-sUJHi65wDwnR276k3A.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 5\n",
    "batch_size = 2\n",
    "lstm = nn.LSTM(input_size=1, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS)\n",
    "lstm_input = torch.randn(sequence_length, batch_size, 1)\n",
    "hidden_0 = torch.randn(NUM_LAYERS, batch_size, HIDDEN_SIZE)\n",
    "cell_state_0 = torch.randn(NUM_LAYERS, batch_size, HIDDEN_SIZE)\n",
    "output, (hn, cn) = lstm(lstm_input, (hidden_0, cell_state_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.shape)  # seq_len, batch_size, hidden_size\n",
    "# containing the output features (h_t) from the last layer of the LSTM, for each t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hn.shape)  # num_layers, batch_size, hidden_size\n",
    "# containing the final hidden state for each element in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cn.shape)  # num_layers, batch_size, hidden_size\n",
    "# containing the final cell state for each element in the sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, out_size):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, hidden_size=hidden_size, num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, out_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        state = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        return hidden, state\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "        all_outputs, hidden = self.lstm(x, hidden)\n",
    "        out = all_outputs[-1]  # We are interested only in the last output\n",
    "        x = self.fc(out)\n",
    "        return x, hidden\n",
    "\n",
    "\n",
    "model = LSTMRegressor(1, HIDDEN_SIZE, 2, 1).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fun = nn.MSELoss()\n",
    "\n",
    "for epoch in range(101):\n",
    "    for x, targets in train_loader:\n",
    "        x = x.to(device).unsqueeze(2)\n",
    "        targets = targets.to(device)\n",
    "        hidden, state = model.init_hidden(x.size(0))\n",
    "        hidden, state = hidden.to(device), state.to(device)\n",
    "        preds, last_hidden = model(x, (hidden, state))\n",
    "        preds = preds.squeeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fun(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}, loss: {loss.item():.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    hidden, state = model.init_hidden(test_data.size(0))\n",
    "    hidden, state = hidden.to(device), state.to(device)\n",
    "    test_preds, _ = model(test_data.to(device).unsqueeze(2), (hidden, state))\n",
    "    print(torch.abs(test_preds.squeeze() - test_targets.to(device)).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 5])\n",
    "plt.plot(test_targets, label=\"original\")\n",
    "plt.plot(test_preds.squeeze().cpu().numpy(), label=\"predictions\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini zadanie: Jak wyglądałyby predykcje w oparciu o poprzednie predykcje?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    hidden, state = model.init_hidden(1)\n",
    "    hidden, state = hidden.to(device), state.to(device)\n",
    "    hidden = (hidden, state)\n",
    "    x_one = test_targets[0:1].unsqueeze(0).to(device)\n",
    "    preds = []\n",
    "    for i in range(len(test_targets)):\n",
    "        x_one, hidden = model(x_one.unsqueeze(0), hidden)\n",
    "        preds.append(x_one.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 5])\n",
    "plt.plot(test_targets, label=\"original\")\n",
    "plt.plot(test_preds.squeeze().cpu().numpy(), label=\"predictions_short_term\")\n",
    "plt.plot(preds, label=\"predictions_long_term\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predykcja Sequence to sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate https://www.galera.ii.pw.edu.pl/~kdeja/data/all_stocks_5yr.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price = pd.read_csv(\"all_stocks_5yr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mastercard_stock = stock_price[stock_price.Name == \"MA\"].open.values\n",
    "visa_stock = stock_price[stock_price.Name == \"V\"].open.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 5])\n",
    "plt.plot(mastercard_stock, label=\"mastercard\")\n",
    "plt.plot(visa_stock, label=\"visa\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_min_value = mastercard_stock.min()\n",
    "m_max_value = mastercard_stock.max()\n",
    "v_min_value = visa_stock.min()\n",
    "v_max_value = visa_stock.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_seq = []\n",
    "data_targets = []\n",
    "sequence_len = 50\n",
    "for i in range(len(mastercard_stock) - sequence_len):\n",
    "    data_seq.append(torch.from_numpy(mastercard_stock[i : i + sequence_len]))\n",
    "    data_targets.append(torch.from_numpy(visa_stock[i : i + sequence_len]))\n",
    "\n",
    "data = (torch.stack(data_seq).float() - m_min_value) / m_max_value\n",
    "data_targets = (torch.stack(data_targets).float() - v_min_value) / v_max_value\n",
    "\n",
    "train_split = int(len(data) * 0.7)\n",
    "train_set = torch.utils.data.TensorDataset(\n",
    "    data[:train_split], data_targets[:train_split]\n",
    ")\n",
    "train_loader = DataLoader(train_set, batch_size=32, drop_last=True)\n",
    "train_set = torch.utils.data.TensorDataset(\n",
    "    data[:train_split], data_targets[:train_split]\n",
    ")\n",
    "train_loader = DataLoader(train_set, batch_size=32, drop_last=True)\n",
    "test_data, test_targets = data[train_split:], data_targets[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Seq_Regressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, out_size):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.proj_size = out_size\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            proj_size=out_size,\n",
    "        )\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.proj_size)\n",
    "        state = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        return hidden, state\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "        all_outputs, hidden = self.lstm(x, hidden)\n",
    "        all_outputs = torch.transpose(all_outputs, 0, 1)\n",
    "        return all_outputs, hidden\n",
    "\n",
    "\n",
    "model = LSTM_Seq_Regressor(1, 50, 2, 1).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fun = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(101):\n",
    "    for x, targets in train_loader:\n",
    "        x = x.to(device).unsqueeze(2)\n",
    "        targets = targets.to(device)\n",
    "        hidden, state = model.init_hidden(x.size(0))\n",
    "        hidden, state = hidden.to(device), state.to(device)\n",
    "        preds, last_hidden = model(x, (hidden, state))\n",
    "        preds = preds.squeeze(2)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fun(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}, loss: {loss.item():.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    selected_test_targets = []\n",
    "    preds = []\n",
    "    for i in range(0, len(test_targets), sequence_len):\n",
    "        hidden, state = model.init_hidden(1)\n",
    "        hidden, state = hidden.to(device), state.to(device)\n",
    "        selected_test_targets.append(test_targets[i])\n",
    "        pred, _ = model(\n",
    "            test_data[i].to(device).unsqueeze(0).unsqueeze(2), (hidden, state)\n",
    "        )\n",
    "        preds.append(pred.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 5])\n",
    "plt.plot(torch.cat(selected_test_targets).cpu().numpy(), label=\"original\")\n",
    "plt.plot(torch.cat(preds).cpu().numpy(), label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co się stało, skąd takie dziwne predykcje?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    selected_test_targets = []\n",
    "    preds = []\n",
    "    hidden, state = model.init_hidden(1)\n",
    "    hidden, state = hidden.to(device), state.to(device)\n",
    "    for i in range(0, len(test_targets), sequence_len):\n",
    "        selected_test_targets.append(test_targets[i])\n",
    "        pred, hidden_out = model(\n",
    "            test_data[i].to(device).unsqueeze(0).unsqueeze(2), (hidden, state)\n",
    "        )\n",
    "        hidden, state = hidden_out\n",
    "        preds.append(pred.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 5])\n",
    "plt.plot(torch.cat(selected_test_targets).cpu().numpy(), label=\"original\")\n",
    "plt.plot(torch.cat(preds).cpu().numpy(), label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klasyfikacja serii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "libras = pd.read_csv(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.data\",\n",
    "    header=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libras.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = libras[90].values\n",
    "data = libras.values[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(classes).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.from_numpy(data).float()\n",
    "data_targets = torch.from_numpy(classes).long()\n",
    "\n",
    "train_indices = rng.random(len(data)) > 0.3\n",
    "test_indices = ~train_indices\n",
    "train_set = torch.utils.data.TensorDataset(\n",
    "    data[train_indices], data_targets[train_indices]\n",
    ")\n",
    "train_loader = DataLoader(train_set, batch_size=32)\n",
    "test_data, test_targets = data[test_indices], data_targets[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Napisz klasyfikator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, hidden_size, num_layers, out_size, bidirectional=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        if bidirectional:\n",
    "            self.bidirectional = 2\n",
    "        else:\n",
    "            self.bidirectional = 1\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size * 90 * self.bidirectional, out_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(\n",
    "            self.num_layers * self.bidirectional, batch_size, self.hidden_size\n",
    "        )\n",
    "        state = torch.zeros(\n",
    "            self.num_layers * self.bidirectional, batch_size, self.hidden_size\n",
    "        )\n",
    "        return hidden, state\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "        all_outputs, hidden = self.lstm(x, hidden)\n",
    "        all_outputs = torch.transpose(all_outputs, 0, 1)\n",
    "        out = torch.flatten(all_outputs, 1)\n",
    "        x = self.fc(out)\n",
    "        return x, hidden\n",
    "\n",
    "\n",
    "model = LSTMRegressor(1, 5, 2, 16, bidirectional=True).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(101):\n",
    "    for x, targets in train_loader:\n",
    "        x = x.to(device).unsqueeze(2)\n",
    "        #         x = x.unsqueeze(2)\n",
    "        targets = targets.to(device)\n",
    "        hidden, state = model.init_hidden(x.size(0))\n",
    "        hidden, state = hidden.to(device), state.to(device)\n",
    "        preds, _ = model(x, (hidden, state))\n",
    "        preds = preds.squeeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fun(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}, loss: {loss.item():.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    hidden, state = model.init_hidden(len(test_data))\n",
    "    hidden, state = hidden.to(device), state.to(device)\n",
    "    preds, _ = model(test_data.to(device).unsqueeze(2), (hidden, state))\n",
    "print(\n",
    "    f\"Accuracy: {(torch.argmax(preds, 1).cpu() == test_targets).sum().item() / len(test_targets):.3}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dane o różnej długości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class VariableLenDataset(Dataset):\n",
    "    def __init__(self, in_data, target):\n",
    "        self.data = [(x, y) for x, y in zip(in_data, target)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        in_data, target = self.data[idx]\n",
    "        return in_data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_gen_val = 10\n",
    "max_gen_val = 1001\n",
    "samples = 1000\n",
    "max_gen_len = 32\n",
    "\n",
    "data = []\n",
    "targets = []\n",
    "max_val = -1\n",
    "for _ in range(samples):\n",
    "    seq_len = rng.integers(low=1, high=max_gen_len, size=1)\n",
    "    data_in = rng.integers(low=min_gen_val, high=max_gen_val, size=seq_len)\n",
    "    data_sum = np.array([data_in[: i + 1].sum() for i in range(len(data_in))])\n",
    "    data.append(torch.from_numpy(data_in))\n",
    "    targets.append(torch.from_numpy(data_sum))\n",
    "    max_val = data_sum[-1] if data_sum[-1] > max_val else max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(x) for x in data[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = int(len(data) * 0.7)\n",
    "data = [(x / max_val).float() for x in data]\n",
    "targets = [(x / max_val).float() for x in targets]\n",
    "train_set = VariableLenDataset(data[:train_indices], targets[:train_indices])\n",
    "test_set = VariableLenDataset(data[train_indices:], targets[train_indices:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "pad = 0\n",
    "\n",
    "\n",
    "def pad_collate(batch, pad_value=0):\n",
    "    xx, yy = zip(*batch)\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    y_lens = [len(y) for y in yy]\n",
    "\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=pad_value)\n",
    "    yy_pad = pad_sequence(yy, batch_first=True, padding_value=pad_value)\n",
    "\n",
    "    return xx_pad, yy_pad, x_lens, y_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_set, batch_size=50, shuffle=True, collate_fn=pad_collate\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_set, batch_size=50, shuffle=False, drop_last=False, collate_fn=pad_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Seq_Regressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, out_size):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.proj_size = out_size\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            proj_size=out_size,\n",
    "        )\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.proj_size)\n",
    "        state = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        return hidden, state\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # x = torch.transpose(x, 0, 1)\n",
    "        all_outputs, hidden = self.lstm(x, hidden)\n",
    "        # all_outputs = torch.transpose(all_outputs, 0, 1)\n",
    "        return all_outputs, hidden\n",
    "\n",
    "\n",
    "model = LSTM_Seq_Regressor(1, 200, 1, 1).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fun = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(101):\n",
    "    for x, targets, x_len, target_len in train_loader:\n",
    "        x = x.to(device).unsqueeze(2)\n",
    "        targets = targets.to(device)\n",
    "        hidden, state = model.init_hidden(x.size(0))\n",
    "        hidden, state = hidden.to(device), state.to(device)\n",
    "\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "        preds, _ = model(x, (hidden, state))\n",
    "        preds = torch.transpose(preds, 0, 1)\n",
    "\n",
    "        #         x_packed = pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n",
    "        #         preds_packed, _ = model(x_packed, (hidden, state))\n",
    "        #         preds, pred_len = pad_packed_sequence(preds_packed, batch_first=True, padding_value=pad)\n",
    "\n",
    "        preds = preds.squeeze(2)\n",
    "        optimizer.zero_grad()\n",
    "        mask = targets != pad\n",
    "        loss = loss_fun(preds[mask], targets[mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}, loss: {loss.item():.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for x, targets, x_len, target_len in test_loader:\n",
    "        x = x.to(device).unsqueeze(2)\n",
    "        targets = targets.to(device)\n",
    "        hidden, state = model.init_hidden(x.shape[0])\n",
    "        hidden, state = hidden.to(device), state.to(device)\n",
    "\n",
    "        #         x = torch.transpose(x, 0, 1)\n",
    "        #         preds, _ = model(x, (hidden, state))\n",
    "        #         preds = torch.transpose(preds, 0, 1)\n",
    "        x_packed = pack_padded_sequence(\n",
    "            x, x_len, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        preds_packed, _ = model(x_packed, (hidden, state))\n",
    "        preds, pred_len = pad_packed_sequence(\n",
    "            preds_packed, batch_first=True, padding_value=pad\n",
    "        )\n",
    "\n",
    "        preds = preds.squeeze(2)\n",
    "        mask_tgt = targets != pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeśli chcemy wykorzystać ostatnie stany ukryte komórki, np. do klasyfikacji, naley wykorzystać `pack_padded_sequence`.\n",
    "Dane nadal pozostaną w odpowiednim formacie, ale RNN nie będzie uwzględniał paddingu (jeśli tego nie zrobimy, RNN zwróci stany ukryty odpowiadający paddingowi, co jest niepozadane).   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
