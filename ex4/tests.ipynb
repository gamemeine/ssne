{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cc72ded",
   "metadata": {},
   "source": [
    "# Image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21f91952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Generator\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abf3d6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa6528d",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e6f0be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and validation directories already exist.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from data import train_val_split\n",
    "\n",
    "base_dir = './data/train'\n",
    "train_dir = './data/preprocessed/train'\n",
    "val_dir = './data/preprocessed/val'\n",
    "\n",
    "val_ratio = 0.2\n",
    "\n",
    "if not os.path.exists(train_dir) and not os.path.exists(val_dir):\n",
    "    train_val_split(base_dir, train_dir, val_dir, val_ratio, random_state) \n",
    "\n",
    "    print(\"Saved train data to:\", train_dir)\n",
    "    print(\"Saved validation data to:\", val_dir)\n",
    "else:\n",
    "    print(\"Train and validation directories already exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6777f43",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8f1f0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 70409\n",
      "Number of validation samples: 8801\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from data import sample_dataset\n",
    "\n",
    "default_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "augmented_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(64, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    # transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_default_ds = ImageFolder(train_dir, transform=default_transform)\n",
    "train_augmented_ds = ImageFolder(train_dir, transform=augmented_transform)\n",
    "train_ds = ConcatDataset([train_default_ds, train_augmented_ds])\n",
    "\n",
    "val_ds = ImageFolder(val_dir, transform=default_transform)\n",
    "\n",
    "# downsample the dataset\n",
    "sample_ratio = 0.5\n",
    "sampled_train_ds = sample_dataset(train_ds, sample_ratio, seed=random_state)\n",
    "sampled_val_ds = sample_dataset(val_ds, sample_ratio, seed=random_state)\n",
    "\n",
    "num_classes = len(train_default_ds.classes)\n",
    "\n",
    "print(f\"Number of training samples: {len(sampled_train_ds)}\")\n",
    "print(f\"Number of validation samples: {len(sampled_val_ds)}\")\n",
    "\n",
    "batch_size = 128\n",
    "num_workers = 0\n",
    "\n",
    "# setup data loaders\n",
    "train_dl = DataLoader(sampled_train_ds,\n",
    "                      batch_size=batch_size,\n",
    "                      num_workers=num_workers,\n",
    "                      shuffle=True,\n",
    "                      generator=Generator().manual_seed(random_state))\n",
    "\n",
    "val_dl = DataLoader(sampled_val_ds,\n",
    "                    batch_size=batch_size,\n",
    "                    num_workers=num_workers, \n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0dedbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample image shape: torch.Size([3, 64, 64])\n",
      "Sample label: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import show_image\n",
    "\n",
    "sample_img, sample_label = train_ds[0]\n",
    "\n",
    "print(f\"Sample image shape: {sample_img.size()}\")\n",
    "print(f\"Sample label: {sample_label}\")\n",
    "\n",
    "show_image(sample_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517c5072",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d21d357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import Trainer\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465789b2",
   "metadata": {},
   "source": [
    "### BasicModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "184c7b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 (training):   1%|▏         | 8/551 [00:01<01:37,  5.54it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m optimizer = optim.Adam(basicNet.to(device).parameters(), lr=\u001b[32m1e-4\u001b[39m, weight_decay=\u001b[32m1e-5\u001b[39m)\n\u001b[32m      7\u001b[39m trainer = Trainer(basicNet, train_dl, val_dl, optimizer, device=device)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\ex4\\training.py:80\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, num_epochs)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_epochs):\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m         train_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m         val_loss, avg_class_acc = \u001b[38;5;28mself\u001b[39m.evaluation_step(epoch, num_epochs)\n\u001b[32m     83\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Avg Class Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_class_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\ex4\\training.py:33\u001b[39m, in \u001b[36mTrainer.train_step\u001b[39m\u001b[34m(self, epoch, num_epochs)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mself\u001b[39m.model.train()\n\u001b[32m     32\u001b[39m total_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m (training)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:350\u001b[39m, in \u001b[36mConcatDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    349\u001b[39m     sample_idx = idx - \u001b[38;5;28mself\u001b[39m.cumulative_sizes[dataset_idx - \u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:245\u001b[39m, in \u001b[36mDatasetFolder.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[33;03m    index (int): Index\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    242\u001b[39m \u001b[33;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    244\u001b[39m path, target = \u001b[38;5;28mself\u001b[39m.samples[index]\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m sample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    247\u001b[39m     sample = \u001b[38;5;28mself\u001b[39m.transform(sample)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:284\u001b[39m, in \u001b[36mdefault_loader\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:263\u001b[39m, in \u001b[36mpil_loader\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpil_loader\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) -> Image.Image:\n\u001b[32m    261\u001b[39m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m         img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m img.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\PIL\\Image.py:3480\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3477\u001b[39m     fp = io.BytesIO(fp.read())\n\u001b[32m   3478\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3480\u001b[39m prefix = \u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3482\u001b[39m preinit()\n\u001b[32m   3484\u001b[39m warning_messages: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] = []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from models import BasicNet\n",
    "\n",
    "basicNet = BasicNet(num_classes=num_classes)\n",
    "\n",
    "optimizer = optim.Adam(basicNet.to(device).parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "trainer = Trainer(basicNet, train_dl, val_dl, optimizer, device=device)\n",
    "trainer.train(num_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f378a19",
   "metadata": {},
   "source": [
    "### Using AvgPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da78941b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 (training): 100%|██████████| 551/551 [01:14<00:00,  7.36it/s]\n",
      "Epoch 1/30 (validation): 100%|██████████| 69/69 [00:10<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: Train Loss: 3.5953, Val Loss: 3.1769, Avg Class Accuracy: 0.1608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 (training):  38%|███▊      | 209/551 [00:29<00:47,  7.14it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m optimizer = optim.Adam(basicNet.to(device).parameters(), lr=\u001b[32m1e-4\u001b[39m, weight_decay=\u001b[32m1e-5\u001b[39m)\n\u001b[32m      7\u001b[39m trainer = Trainer(basicNet, train_dl, val_dl, optimizer, device=device)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\ex4\\training.py:80\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, num_epochs)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_epochs):\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m         train_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m         val_loss, avg_class_acc = \u001b[38;5;28mself\u001b[39m.evaluation_step(epoch, num_epochs)\n\u001b[32m     83\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Avg Class Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_class_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\ex4\\training.py:44\u001b[39m, in \u001b[36mTrainer.train_step\u001b[39m\u001b[34m(self, epoch, num_epochs)\u001b[39m\n\u001b[32m     41\u001b[39m     loss.backward()\n\u001b[32m     42\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * images.size(\u001b[32m0\u001b[39m)\n\u001b[32m     46\u001b[39m epoch_loss = total_loss / \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.train_dl.dataset)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m epoch_loss\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from models import AVGPoolNet_Kernel5\n",
    "\n",
    "basicNet = AVGPoolNet_Kernel5(num_classes=num_classes)\n",
    "\n",
    "optimizer = optim.Adam(basicNet.to(device).parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "trainer = Trainer(basicNet, train_dl, val_dl, optimizer, device=device)\n",
    "trainer.train(num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b027faf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 (training): 100%|██████████| 551/551 [01:13<00:00,  7.52it/s]\n",
      "Epoch 1/30 (validation): 100%|██████████| 69/69 [00:09<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: Train Loss: 3.5890, Val Loss: 3.1593, Avg Class Accuracy: 0.1632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 (training): 100%|██████████| 551/551 [01:13<00:00,  7.45it/s]\n",
      "Epoch 2/30 (validation): 100%|██████████| 69/69 [00:09<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: Train Loss: 3.2124, Val Loss: 2.9151, Avg Class Accuracy: 0.2320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 (training):   1%|          | 4/551 [00:00<01:46,  5.13it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m optimizer = optim.Adam(basicNet.to(device).parameters(), lr=\u001b[32m1e-4\u001b[39m, weight_decay=\u001b[32m1e-5\u001b[39m)\n\u001b[32m      7\u001b[39m trainer = Trainer(basicNet, train_dl, val_dl, optimizer, device=device)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\ex4\\training.py:80\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, num_epochs)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_epochs):\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m         train_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m         val_loss, avg_class_acc = \u001b[38;5;28mself\u001b[39m.evaluation_step(epoch, num_epochs)\n\u001b[32m     83\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Avg Class Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_class_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\ex4\\training.py:33\u001b[39m, in \u001b[36mTrainer.train_step\u001b[39m\u001b[34m(self, epoch, num_epochs)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mself\u001b[39m.model.train()\n\u001b[32m     32\u001b[39m total_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m (training)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:350\u001b[39m, in \u001b[36mConcatDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    349\u001b[39m     sample_idx = idx - \u001b[38;5;28mself\u001b[39m.cumulative_sizes[dataset_idx - \u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:247\u001b[39m, in \u001b[36mDatasetFolder.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    245\u001b[39m sample = \u001b[38;5;28mself\u001b[39m.loader(path)\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     sample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    249\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[39m, in \u001b[36mNormalize.forward\u001b[39m\u001b[34m(self, tensor)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) -> Tensor:\n\u001b[32m    270\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[33;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m \u001b[33;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:350\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch.Tensor):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:920\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    917\u001b[39m     tensor = tensor.clone()\n\u001b[32m    919\u001b[39m dtype = tensor.dtype\n\u001b[32m--> \u001b[39m\u001b[32m920\u001b[39m mean = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    921\u001b[39m std = torch.as_tensor(std, dtype=dtype, device=tensor.device)\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (std == \u001b[32m0\u001b[39m).any():\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from models import AVGPoolNet_Kernel3\n",
    "\n",
    "basicNet = AVGPoolNet_Kernel3(num_classes=num_classes)\n",
    "\n",
    "optimizer = optim.Adam(basicNet.to(device).parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "trainer = Trainer(basicNet, train_dl, val_dl, optimizer, device=device)\n",
    "trainer.train(num_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04f1af9",
   "metadata": {},
   "source": [
    "#### Not using pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c28fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 (training): 100%|██████████| 551/551 [01:10<00:00,  7.83it/s]\n",
      "Epoch 1/30 (validation): 100%|██████████| 69/69 [00:09<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: Train Loss: 3.6755, Val Loss: 3.3080, Avg Class Accuracy: 0.1426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 (training):   4%|▎         | 20/551 [00:02<01:15,  7.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m optimizer = optim.Adam(basicNet.to(device).parameters(), lr=\u001b[32m1e-4\u001b[39m, weight_decay=\u001b[32m1e-5\u001b[39m)\n\u001b[32m      7\u001b[39m trainer = Trainer(basicNet, train_dl, val_dl, optimizer, device=device)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\ex4\\training.py:80\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, num_epochs)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_epochs):\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m         train_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m         val_loss, avg_class_acc = \u001b[38;5;28mself\u001b[39m.evaluation_step(epoch, num_epochs)\n\u001b[32m     83\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Avg Class Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_class_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\ex4\\training.py:33\u001b[39m, in \u001b[36mTrainer.train_step\u001b[39m\u001b[34m(self, epoch, num_epochs)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mself\u001b[39m.model.train()\n\u001b[32m     32\u001b[39m total_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m (training)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:350\u001b[39m, in \u001b[36mConcatDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    349\u001b[39m     sample_idx = idx - \u001b[38;5;28mself\u001b[39m.cumulative_sizes[dataset_idx - \u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:247\u001b[39m, in \u001b[36mDatasetFolder.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    245\u001b[39m sample = \u001b[38;5;28mself\u001b[39m.loader(path)\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     sample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    249\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projekty\\ssne\\venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:712\u001b[39m, in \u001b[36mRandomHorizontalFlip.forward\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m    705\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    706\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    707\u001b[39m \u001b[33;03m        img (PIL Image or Tensor): Image to be flipped.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    710\u001b[39m \u001b[33;03m        PIL Image or Tensor: Randomly flipped image.\u001b[39;00m\n\u001b[32m    711\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m712\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m < \u001b[38;5;28mself\u001b[39m.p:\n\u001b[32m    713\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m F.hflip(img)\n\u001b[32m    714\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from models import No_PoolNet\n",
    "\n",
    "basicNet = No_PoolNet(num_classes=num_classes)\n",
    "\n",
    "optimizer = optim.Adam(basicNet.to(device).parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "trainer = Trainer(basicNet, train_dl, val_dl, optimizer, device=device)\n",
    "trainer.train(num_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39016aa",
   "metadata": {},
   "source": [
    "### Renset-like model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d69386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import SimpleResNet\n",
    "\n",
    "simpleResNet = SimpleResNet(num_classes=num_classes)\n",
    "\n",
    "optimizer = optim.Adam(simpleResNet.to(device).parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "trainer = Trainer(simpleResNet, train_dl, val_dl, optimizer, device=device)\n",
    "trainer.train(num_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657afee3",
   "metadata": {},
   "source": [
    "### Four-block CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ec7286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import FourBlockCNN\n",
    "\n",
    "fourBlockCNN = FourBlockCNN(num_classes=50)\n",
    "\n",
    "optimizer = optim.Adam(fourBlockCNN.to(device).parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "trainer = Trainer(fourBlockCNN, train_dl, val_dl, optimizer, device=device)\n",
    "trainer.train(num_epochs=30)\n",
    "\n",
    "# Results:\n",
    "# Without augmentation: Epoch 10/10: Train Loss: 2.2281, Val Loss: 2.4054, Avg Class Accuracy: 0.3387\n",
    "# With augmentation: Epoch 10/10: Train Loss: 1.7477, Val Loss: 1.8611, Avg Class Accuracy: 0.4847"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d66a126",
   "metadata": {},
   "source": [
    "## Resnet18\n",
    "\n",
    "Here we train the ResNet18 model on the dataset to see the state of the art performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7b45fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "resnet18 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n",
    "\n",
    "in_features = resnet18.fc.in_features\n",
    "resnet18.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "resnet18 = resnet18.to(device)\n",
    "\n",
    "optimizer = optim.Adam(resnet18.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "trainer = Trainer(resnet18, train_dl, val_dl, optimizer, device=device)\n",
    "trainer.train(num_epochs=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssne",
   "language": "python",
   "name": "ssne"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
